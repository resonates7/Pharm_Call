{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4a22aa-024d-4635-8e13-0e3ae5cad8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q eli5 transformers\n",
    "from eli5.lime import TextExplainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "\n",
    "def model_adapter(texts: List[str]):\n",
    "    all_scores = []\n",
    "    for i in range(0, len(texts), 64):\n",
    "        batch = texts[i:i+64]\n",
    "        # use bert encoder to tokenize text \n",
    "        encoded_input = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, \n",
    "                                  max_length=model.config.max_position_embeddings-2).to(device)\n",
    "        # run the model\n",
    "        output = model(**encoded_input)\n",
    "\\        scores = output[0].cpu().softmax(1).detach().numpy()\n",
    "        all_scores.extend(scores)\n",
    "    return np.array(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7a5d573-293b-424c-a5a5-f725e25db04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tejomay.gadgil/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/tejomay.gadgil/miniforge3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=POSITIVE\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>1.000</b>, score <b>9.562</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +9.426\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.97%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.136\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 80.98%); opacity: 0.87\" title=\"0.952\">thank</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.33%); opacity: 0.83\" title=\"0.362\">you</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 96.17%); opacity: 0.81\" title=\"0.097\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.01%); opacity: 0.83\" title=\"-0.435\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.47%); opacity: 0.83\" title=\"-0.465\">looking</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.61%); opacity: 0.83\" title=\"-0.458\">at</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.16%); opacity: 0.85\" title=\"-0.668\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.65%); opacity: 0.83\" title=\"-0.399\">label</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.76%); opacity: 0.82\" title=\"-0.288\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.03%); opacity: 0.83\" title=\"-0.378\">it</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.43%); opacity: 0.85\" title=\"-0.715\">was</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-2.752\">not</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.68%); opacity: 0.86\" title=\"-0.764\">making</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.26%); opacity: 0.85\" title=\"-0.726\">sense</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.36%); opacity: 0.81\" title=\"-0.090\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.54%); opacity: 0.84\" title=\"0.520\">me</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 74.27%); opacity: 0.91\" title=\"1.466\">but</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.24%); opacity: 0.88\" title=\"1.078\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.22%); opacity: 0.89\" title=\"1.232\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 74.75%); opacity: 0.90\" title=\"1.426\">appreciate</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.47%); opacity: 0.84\" title=\"0.525\">your</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.58%); opacity: 0.81\" title=\"0.202\">help</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 77.98%); opacity: 0.89\" title=\"1.173\">you</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(120, 100.00%, 84.76%); opacity: 0.85\" title=\"0.693\">re</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.49%); opacity: 0.87\" title=\"0.987\">very</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.18%); opacity: 0.96\" title=\"2.258\">pleasant</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.51%); opacity: 0.82\" title=\"0.301\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.68%); opacity: 0.86\" title=\"0.832\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 69.75%); opacity: 0.93\" title=\"1.847\">like</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.66%); opacity: 0.82\" title=\"0.244\">that</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"SGDClassifier(alpha=0.001, loss='log', penalty='elasticnet',\\n              random_state=RandomState(MT19937) at 0x29E325740)\", description=None, error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target='POSITIVE', feature_weights=FeatureWeights(pos=[FeatureWeight(feature='you', weight=1.7430218719301034, std=None, value=2.0), FeatureWeight(feature='pleasant', weight=1.6582230316858455, std=None, value=1.0), FeatureWeight(feature='appreciate', weight=1.4264378159313833, std=None, value=1.0), FeatureWeight(feature='like', weight=1.1789262252090231, std=None, value=1.0), FeatureWeight(feature='thank', weight=1.1782974572013116, std=None, value=1.0), FeatureWeight(feature='i', weight=1.1378736063099812, std=None, value=3.0), FeatureWeight(feature='sure', weight=1.0810672206102623, std=None, value=1.0), FeatureWeight(feature='i like', weight=0.6382156047892991, std=None, value=1.0), FeatureWeight(feature='very', weight=0.5612679228547159, std=None, value=1.0), FeatureWeight(feature='me but', weight=0.5536605414790101, std=None, value=1.0), FeatureWeight(feature='but i', weight=0.5483098793594202, std=None, value=1.0), FeatureWeight(feature='your', weight=0.5245576927175194, std=None, value=1.0), FeatureWeight(feature='making', weight=0.524282225553592, std=None, value=1.0), FeatureWeight(feature='pleasant and', weight=0.3955498450955036, std=None, value=1.0), FeatureWeight(feature='but', weight=0.36378442291365454, std=None, value=1.0), FeatureWeight(feature='to', weight=0.33702529882442284, std=None, value=1.0), FeatureWeight(feature='sense', weight=0.33379243948021164, std=None, value=1.0), FeatureWeight(feature='re', weight=0.28747452937962464, std=None, value=1.0), FeatureWeight(feature='re very', weight=0.2217456847574038, std=None, value=1.0), FeatureWeight(feature='that', weight=0.21464884316592606, std=None, value=1.0), FeatureWeight(feature='very pleasant', weight=0.20373231496124697, std=None, value=1.0), FeatureWeight(feature='you re', weight=0.18408321028616254, std=None, value=1.0), FeatureWeight(feature='and', weight=0.180159326281075, std=None, value=2.0), FeatureWeight(feature='i sure', weight=0.15063644452203914, std=None, value=1.0), FeatureWeight(feature='<BIAS>', weight=0.13634550612662572, std=None, value=1.0), FeatureWeight(feature='help you', weight=0.11744857361667349, std=None, value=1.0), FeatureWeight(feature='help', weight=0.08429394928528455, std=None, value=1.0), FeatureWeight(feature='like that', weight=0.029690290712990217, std=None, value=1.0)], neg=[FeatureWeight(feature='not', weight=-1.526484034441103, std=None, value=1.0), FeatureWeight(feature='not making', weight=-0.6554063129369875, std=None, value=1.0), FeatureWeight(feature='making sense', weight=-0.6333117299543326, std=None, value=1.0), FeatureWeight(feature='was not', weight=-0.5705154872382412, std=None, value=1.0), FeatureWeight(feature='sense to', weight=-0.4266797029336293, std=None, value=1.0), FeatureWeight(feature='the label', weight=-0.39895422718912765, std=None, value=1.0), FeatureWeight(feature='and it', weight=-0.3782976524284424, std=None, value=1.0), FeatureWeight(feature='was looking', weight=-0.29065078520448323, std=None, value=1.0), FeatureWeight(feature='was', weight=-0.28847855034157543, std=None, value=2.0), FeatureWeight(feature='you i', weight=-0.28272235279195, std=None, value=1.0), FeatureWeight(feature='at the', weight=-0.26894700797668747, std=None, value=1.0), FeatureWeight(feature='thank you', weight=-0.22677346134260876, std=None, value=1.0), FeatureWeight(feature='and i', weight=-0.1850554973962562, std=None, value=1.0), FeatureWeight(feature='looking at', weight=-0.09642759464391816, std=None, value=1.0), FeatureWeight(feature='at', weight=-0.09238993004709191, std=None, value=1.0), FeatureWeight(feature='looking', weight=-0.07827211914959596, std=None, value=1.0), FeatureWeight(feature='me', weight=-0.0332943322959536, std=None, value=1.0)], pos_remaining=0, neg_remaining=0), proba=0.9999296453271607, score=9.561890996728325, weighted_spans=WeightedSpans(docs_weighted_spans=[DocWeightedSpans(document=\"thank you, i was looking at the label and it was not making sense to me. but i sure appreciate your help, you're very pleasant and i like that.\", spans=[('thank', [(0, 5)], 1.1782974572013116), ('you', [(6, 9)], 1.7430218719301034), ('i', [(11, 12)], 1.1378736063099812), ('was', [(13, 16)], -0.28847855034157543), ('looking', [(17, 24)], -0.07827211914959596), ('at', [(25, 27)], -0.09238993004709191), ('and', [(38, 41)], 0.180159326281075), ('was', [(45, 48)], -0.28847855034157543), ('not', [(49, 52)], -1.526484034441103), ('making', [(53, 59)], 0.524282225553592), ('sense', [(60, 65)], 0.33379243948021164), ('to', [(66, 68)], 0.33702529882442284), ('me', [(69, 71)], -0.0332943322959536), ('but', [(73, 76)], 0.36378442291365454), ('i', [(77, 78)], 1.1378736063099812), ('sure', [(79, 83)], 1.0810672206102623), ('appreciate', [(84, 94)], 1.4264378159313833), ('your', [(95, 99)], 0.5245576927175194), ('help', [(100, 104)], 0.08429394928528455), ('you', [(106, 109)], 1.7430218719301034), ('re', [(110, 112)], 0.28747452937962464), ('very', [(113, 117)], 0.5612679228547159), ('pleasant', [(118, 126)], 1.6582230316858455), ('and', [(127, 130)], 0.180159326281075), ('i', [(131, 132)], 1.1378736063099812), ('like', [(133, 137)], 1.1789262252090231), ('that', [(138, 142)], 0.21464884316592606), ('thank you', [(0, 5), (6, 9)], -0.22677346134260876), ('you i', [(6, 9), (11, 12)], -0.28272235279195), ('was looking', [(13, 16), (17, 24)], -0.29065078520448323), ('looking at', [(17, 24), (25, 27)], -0.09642759464391816), ('at the', [(25, 27), (28, 31)], -0.26894700797668747), ('the label', [(28, 31), (32, 37)], -0.39895422718912765), ('and it', [(38, 41), (42, 44)], -0.3782976524284424), ('was not', [(45, 48), (49, 52)], -0.5705154872382412), ('not making', [(49, 52), (53, 59)], -0.6554063129369875), ('making sense', [(53, 59), (60, 65)], -0.6333117299543326), ('sense to', [(60, 65), (66, 68)], -0.4266797029336293), ('me but', [(69, 71), (73, 76)], 0.5536605414790101), ('but i', [(73, 76), (77, 78)], 0.5483098793594202), ('i sure', [(77, 78), (79, 83)], 0.15063644452203914), ('help you', [(100, 104), (106, 109)], 0.11744857361667349), ('you re', [(106, 109), (110, 112)], 0.18408321028616254), ('re very', [(110, 112), (113, 117)], 0.2217456847574038), ('very pleasant', [(113, 117), (118, 126)], 0.20373231496124697), ('pleasant and', [(118, 126), (127, 130)], 0.3955498450955036), ('and i', [(127, 130), (131, 132)], -0.1850554973962562), ('i like', [(131, 132), (133, 137)], 0.6382156047892991), ('like that', [(133, 137), (138, 142)], 0.029690290712990217)], preserve_density=False, vec_name=None)], other=FeatureWeights(pos=[FeatureWeight(feature=<FormattedFeatureName 'Highlighted in text (sum)'>, weight=9.425545490601705, std=None, value=None), FeatureWeight(feature='<BIAS>', weight=0.13634550612662572, std=None, value=1.0)], neg=[], pos_remaining=0, neg_remaining=0)), heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TextExplainer(n_samples = 5000)\n",
    "te.fit(\"\"\"Thank you, I was looking at the label and it was not making sense to me. But I sure appreciate your help, you're very pleasant and I like that.\"\"\", model_adapter)\n",
    "te.explain_prediction(target_names=list(model.config.id2label.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
